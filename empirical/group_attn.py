# -*- coding: utf-8 -*-
#ATTENTION IS LIKE ACTUAL ATTENTION NOT GNN ATTENTION LOL
"""group_focused_attention_events.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krgbSFvvxaahQtzA1yJnzP6HW2XEcPCq
"""

import pickle
import torch
import numpy as np
import math
import os
import io
from itertools import combinations
import time
from util import read_pickle, apply_transform, get_time_slice, preprocess

"""
WALKTHROUGH:

The data is preprocessed to the following format:
{
  frame_number (int) : {
    tracker_id (int) : [T, J, 3]
  }
}

#to detect length of collision
HOW IT WORKS:
1. sliding window across all frames
2. at any given frame if two individuals are focused on the same spot, increment window size
3. if window breaks:
  a. window is exceeds min threshold, qualifies as group focus -> add seconds
  b. reset window size

you can EDIT the minimum number of frames that it takes to qualify as group focus, and the distance between vectors BELOW
"""

def compute_gaze_vector(joints):
    """
    Computes the gaze direction vector for a single person's joint data.

    Expects joints to be a numpy array of shape [J, 3] where the joints at indices
    57 (left eye), 56 (right eye), and 12 (neck) are used.

    Returns:
        (origin, gaze_vector): A tuple where `origin` is the midpoint between the eyes,
                               and `gaze_vector` is the corrected, normalized gaze direction.
    """
    # Joints and Init 
    L = joints[57]
    R = joints[56]
    N = joints[12]
    mu = (L + R) / 2  # Midpoint between eyes
    initial_gaze = mu - N

    # Define a plane using the vector between the eyes and the initial gaze
    eye_vector = L - R
    plane_normal = np.cross(eye_vector, initial_gaze)
    plane_normal = plane_normal / np.linalg.norm(plane_normal)

    # Projection of the initial gaze onto the plane
    corrected_gaze = initial_gaze - np.dot(initial_gaze, plane_normal) * plane_normal
    corrected_gaze = corrected_gaze / np.linalg.norm(corrected_gaze)
    return mu, corrected_gaze

"""
3 frames => 1 second
MIN TIME SLICE: [5s] 15s 30s

sliding window across all frames
track window size
at any given frame if two individuals are focused on the same spot, increment window size
if window breaks:
  1. window is exceeds min threshold, qualifies as group focus -> add seconds
  2. reset window size

return array of six [0, 0, 0, 0, 0, 0] where each index in the total time focused in that phase

metric should include percentage of time in each phase of focus
"""

def group_focused_attention(data, min_frames, distance_between_vectors_threshold):
  #use a sliding window to detect whether mutliple people are focusing over the same spot"

  """
        distance_between_vectors_threshold: The maximum allowed distance between gaze vectors for them to be considered focused on the same spot."""
  """
  parameters:
  - data: array of dictionaries tracker_id : joints3d

  notes: pad the time for easier window time slice
  """
  group_focus_time = 0

  # sliding window
  l = 0
  r = 0
  event_record = [0 for _ in range(len(data))] #memset
  unique_source_record = [0 for _ in range(len(data))] #memset

  for i, tracker_data in enumerate(data): #loop over each frame
    focused_status, num_unique_sources = is_group_focused(tracker_data, distance_between_vectors_threshold)
    unique_source_record[i] = num_unique_sources
    if focused_status:
        r = i #extend the sliding window
    else:
        if (r - l >= min_frames): #if it was long enough
            group_focus_time += (r - l) 
            for frame_idx in range(l, r):
                event_record[frame_idx] += 1
        l = i #reset sliding window
        r = i #reset sliding window

  #Transform unique source record to account for focus lapses 
  for i in range(len(unique_source_record)):

    #adjust for focus lapses
    unique_source_record[i] = event_record[i] * unique_source_record[i] 

  return group_focus_time, event_record, unique_source_record



#checks if two people are focused on the same spot
def is_group_focused(trackers, distance_between_vectors_threshold):
    gaze_vectors = []
    for tracker_id, tracker_data in trackers.items(): #for each person caclculate eye dist
        midpoint, gaze_vector = compute_gaze_vector(tracker_data)
        gaze_vectors.append([tracker_id, midpoint, gaze_vector])

    min_distance = float('inf')
    epsilon = 1e-6
    matched_trackers = [] 

    """Iterates over all possible pairs (t1, O1, v1), (t2, O2, v2), where:
        t1, t2: The two tracker IDs.
        O1, O2: The midpoints of their eyes.
        v1, v2: Their gaze vectors."""
    for (t1, O1, v1), (t2, O2, v2) in combinations(gaze_vectors, 2):
        v1 = v1 / np.linalg.norm(v1)
        v2 = v2 / np.linalg.norm(v2)

        # Vector between the two origins
        delta_O = O2 - O1

        # Cross product of direction vectors
        v1_cross_v2 = np.cross(v1, v2) #are they looking at the same spot?

        norm_v1_cross_v2 = np.linalg.norm(v1_cross_v2)

        if norm_v1_cross_v2 < epsilon: #out of bounds - not looking
            # Parallel case: Compute perpendicular distance
            perp_distance = np.linalg.norm(delta_O - np.dot(delta_O, v1) * v1)
        else:
            # Compute the minimum distance using the formula for skew lines
            perp_distance = np.abs(np.dot(delta_O, v1_cross_v2)) / norm_v1_cross_v2
        if perp_distance <= distance_between_vectors_threshold: #looking at the same distance
            matched_trackers.append((t1, t2))
        min_distance = min(min_distance, perp_distance)
    
    #Count uniqu sources of attention 
    tracker_set = set() 
    num_unique_sources = 0 
    for tk1, tk2 in matched_trackers:
        if tk1 not in tracker_set and tk2 not in tracker_set:
            num_unique_sources += 1    
        tracker_set.add(tk1)
        tracker_set.add(tk2)

    return (min_distance <= distance_between_vectors_threshold, num_unique_sources)


def tester():
    frames = ['frame_000000.pkl', 'frame_009790.pkl']
    time_slice = get_time_slice(frames[0], frames[1], "/content/drive/MyDrive/research/joint_out/", 10)
    time_slice = preprocess(time_slice)
    data = []

    for frame in time_slice:
        tracker_dict = {}
        for i, tracker_id in enumerate(frame['trackers']):
            person_joints = frame['joints3d'][i]
            tracker_dict[tracker_id] = person_joints
        data.append(tracker_dict)

    print("Data processed")
    time_start = time.time()
    group_focused_time = group_focused_attention(data)
    time_end = time.time()
    print(f"Execution time: {time_end - time_start} seconds")
    print(group_focused_time)

def pipeline_tester():
    folder_path = "/pasteur/data/ghent_surg/full_hmr_outputs/220610_22011_test_5000/joint_out/"
    time_slice = get_time_slice(0, 0, folder_path, debug=True)
    time_slice = preprocess(time_slice)
    data = [] 

    # Hyperparameters
    FPS = 3
    MIN_FRAMES = 15
    DISTANCE_BETWEEN_VECTORS_THRESHOLD = 3 

    for frame in time_slice:
        tracker_dict = {}
        for i, tracker_id in enumerate(frame['trackers']):
            person_joints = frame['joints3d'][i]
            tracker_dict[tracker_id] = person_joints
        data.append(tracker_dict)

    print("Data processed")
    time_start = time.time()
    group_focused_time, event_record, unique_source_record = group_focused_attention(data, MIN_FRAMES, DISTANCE_BETWEEN_VECTORS_THRESHOLD)
    time_end = time.time()
    print(f"Execution time: {time_end - time_start} seconds")
    print(len(unique_source_record))
    print(len(event_record))
    print(len(data)) 
    print(group_focused_time)
    breakpoint() 

if __name__ == "__main__":
    pipeline_tester()
    

